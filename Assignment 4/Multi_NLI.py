# # -*- coding: utf-8 -*-
# """nliELMO (1).ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/15D0PhTULLFtZfIDZW1L6UVY42wPEtZt4
# """

# #mount to google drive
# from google.colab import drive
# drive.mount('/content/drive')

# !pip install datasets

import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from datasets import load_dataset
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torchtext
# Importing Dataset and DataLoader
from torch.utils.data import Dataset, DataLoader
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
dataset = load_dataset("multi_nli")
punctuations = set(string.punctuation)

dataset

train = []
for i, example in enumerate(dataset['train'].select(range(80000))):
    sentence = dataset['train'][i]['premise'] + " " + dataset['train'][i]['hypothesis']
    sentence = sentence.lower()
    sentence = word_tokenize(sentence)
    sentence = [word for word in sentence if word not in stop_words]
    sentence = [word for word in sentence if word not in punctuations]
    label = dataset['train'][i]['label']
    train.append({"sentence": sentence, "label": label})    

test = []
for i in range(len(dataset['train'].select(range(80000, 100000)))):
    sentence = dataset['train'][i]['premise'] + " " + dataset['train'][i]['hypothesis']
    sentence = sentence.lower()
    sentence = word_tokenize(sentence)
    sentence = [word for word in sentence if word not in stop_words]
    sentence = [word for word in sentence if word not in punctuations]
    label = dataset['train'][i]['label']
    test.append({"sentence": sentence, "label": label})


val_mismatch = []
for i in range(len(dataset['validation_mismatched'])):
    sentence = dataset['validation_mismatched'][i]['premise'] + " " + dataset['validation_mismatched'][i]['hypothesis']
    sentence = sentence.lower()
    sentence = word_tokenize(sentence)
    sentence = [word for word in sentence if word not in stop_words]
    sentence = [word for word in sentence if word not in punctuations]
    label = dataset['validation_mismatched'][i]['label']
    val_mismatch.append({"sentence": sentence, "label": label})

val_matched = []
for i in range(len(dataset['validation_matched'])):
    sentence = dataset['validation_matched'][i]['premise'] + " " + dataset['validation_matched'][i]['hypothesis']
    sentence = sentence.lower()
    sentence = word_tokenize(sentence)
    sentence = [word for word in sentence if word not in stop_words]
    sentence = [word for word in sentence if word not in punctuations]
    label = dataset['validation_matched'][i]['label']
    val_matched.append({"sentence": sentence, "label": label})

val = {"mismatch": val_mismatch, "matched": val_matched}

print(len(train))
print(len(test))
print(len(val_mismatch))
print(len(val_matched))

# print 420th example
# print(train[420])

def create_vocab(tokenized_dataset, freq_threshold=3):
    # Create a dictionary to hold the word frequency counts
    word_counts = {}

    # Count the frequency of each word in the dataset
    for example in tokenized_dataset:
        for word in example['sentence']:
            if word in word_counts:
                word_counts[word] += 1
            else:
                word_counts[word] = 1

    # Create a vocabulary of words that occur at least freq_threshold times
    vocab = {'<PAD>': 0, '<UNK>': 1}
    index = 2
    for word, count in word_counts.items():
        if count >= freq_threshold:
            vocab[word] = index
            index += 1

    return vocab


def token2index_dataset(tokenized_dataset, vocab):
    # Convert each sentence to a list of indices using the vocabulary
    indexed_dataset = []
    for example in tokenized_dataset:
        indexed_sentence = [vocab.get(word, 1) for word in example['sentence']]
        indexed_dataset.append(
            {'sentence': indexed_sentence, 'label': example['label']})

    return indexed_dataset


# Create the vocabulary
vocab = create_vocab(train)
print(len(vocab))

# Convert the tokenized datasets to indices
train_data = token2index_dataset(train, vocab)
valid_data_mismatch = token2index_dataset(val_mismatch, vocab)
valid_data_matched = token2index_dataset(val_matched, vocab)
val = {"mismatch": valid_data_mismatch, "matched": valid_data_matched}
test_data = token2index_dataset(test, vocab)

MAX_LENGTH = 120


def pad_sentences(sentences, max_length=120, pad_index=0):
    padded_sentences = []
    for sentence in sentences:
        if len(sentence) >= max_length:
            padded_sentences.append(sentence[:max_length])
        else:
            num_padding = max_length - len(sentence)
            new_sentence = sentence + [pad_index] * num_padding
            padded_sentences.append(new_sentence)
    return padded_sentences


train_data_labels = [example['label'] for example in train_data]
train_data_sentences = [example['sentence'] for example in train_data]
train_data_sentences = pad_sentences(train_data_sentences, MAX_LENGTH)

valid_data_labels_mismatch = [example['label'] for example in valid_data_mismatch]
valid_data_sentences_mismatch = [example['sentence'] for example in valid_data_mismatch]
valid_data_sentences_mismatch = pad_sentences(valid_data_sentences_mismatch, MAX_LENGTH)

valid_data_labels_matched = [example['label'] for example in valid_data_matched]
valid_data_sentences_matched = [example['sentence'] for example in valid_data_matched]
valid_data_sentences_matched = pad_sentences(valid_data_sentences_matched, MAX_LENGTH)

val = {"mismatch": {"labels": valid_data_labels_mismatch, "sentences": valid_data_sentences_mismatch}, "matched": {"labels": valid_data_labels_matched, "sentences": valid_data_sentences_matched}}

test_data_labels = [example['label'] for example in test_data]
test_data_sentences = [example['sentence'] for example in test_data]
test_data_sentences = pad_sentences(test_data_sentences, MAX_LENGTH)

# print lengths
print(len(train_data_sentences))
print(len(valid_data_sentences_mismatch))
print(len(valid_data_sentences_matched))
print(len(test_data_sentences))

class DatasetConstructor(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
        self.backward_data = [sentence[:-1] for sentence in self.data]
        self.forward_data = [sentence[1:] for sentence in self.data]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return torch.tensor(self.data[index]), torch.tensor(self.forward_data[index]), torch.tensor(self.backward_data[index]), torch.tensor(self.labels[index])


train_dataset = DatasetConstructor(train_data_sentences, train_data_labels)
valid_dataset_mismatch = DatasetConstructor(valid_data_sentences_mismatch, valid_data_labels_mismatch)
valid_dataset_matched = DatasetConstructor(valid_data_sentences_matched, valid_data_labels_matched)

val = {"mismatch": valid_dataset_mismatch, "matched": valid_dataset_matched}

test_dataset = DatasetConstructor(test_data_sentences, test_data_labels)

# Create the DataLoader
BATCH_SIZE = 32
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
valid_loader_mismatch = DataLoader(valid_dataset_mismatch, batch_size=BATCH_SIZE, shuffle=True)
valid_loader_matched = DataLoader(valid_dataset_matched, batch_size=BATCH_SIZE, shuffle=True)
val = {"mismatch": valid_loader_mismatch, "matched": valid_loader_matched}
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)

# Load the GloVe embeddings
glove_vectors = torchtext.vocab.GloVe(name='6B', dim=100)

# Create a weight matrix for words in the training dataset
weights_matrix = torch.zeros((len(vocab), 100))
words_found = 0
for i, word in enumerate(vocab.keys()):
    try:
        weights_matrix[i] = glove_vectors[word]
        words_found += 1
    except KeyError:
        weights_matrix[i] = torch.zeros(100)

import torch.nn.functional as F
import torch.nn as nn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class ELMo(nn.Module):
    def __init__(self, vocab_size, weights_matrix, hidden_size=100, num_layers=2, dropout=0.2):
        super(ELMo, self).__init__()

        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, 100)
        self.embedding.weight.data.copy_(torch.FloatTensor(weights_matrix))
        self.embedding.weight.requires_grad = True

        # LSTM layers
        self.lstm_forward = nn.LSTM(
            100, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)
        self.lstm_backward = nn.LSTM(
            100, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)

        # Dropout layer
        self.dropout = nn.Dropout(p=dropout)

        # Fully connected layer
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x_forward, x_backward):
        # Embedding layer
        x_forward = self.embedding(x_forward)
        x_backward = self.embedding(x_backward)

        # print("1 ", x_backward.shape)

        # LSTM layers
        out_forward, _ = self.lstm_forward(x_backward)
        # print("2 ", out_forward.shape)
        out_backward, _ = self.lstm_backward(out_forward)
        # print("3 ", out_backward.shape)

        out = self.fc(out_backward)

        return out


# Instantiate the ELMo model
elmo_model = ELMo(len(vocab), weights_matrix).to(device)
print(elmo_model)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(elmo_model.parameters(), lr=0.001)

## TRAINING
# Define the training loop
def train_loop(model, criterion, optimizer, train_loader, valid_loader_mismatch, valid_loader_matched, num_epochs):
    train_losses = []
    valid_losses_mismatch = []
    valid_losses_matched = []
    train_acc = []
    valid_acc_mismatch = []
    valid_acc_matched = []

    for epoch in tqdm(range(num_epochs)):
        # Train the model
        train_loss = 0.0
        correct_train = 0
        total_train = 0
        model.train()
        for _, x_forward, x_backward, labels in tqdm(train_loader):
            optimizer.zero_grad()
            outputs = model(x_forward.to(device), x_backward.to(device))
            outputs = outputs.to(device)
            outputs = outputs.view(-1, len(vocab))
            target = x_forward.view(-1).to(device)
            loss = criterion(outputs, target)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total_train += target.size(0)
            correct_train += (predicted == target).sum().item()

        train_acc_epoch = correct_train / total_train
        train_acc.append(train_acc_epoch)
        train_loss /= len(train_loader.dataset)
        train_losses.append(train_loss)

        # Evaluate the model
        valid_loss_mismatch = 0.0
        valid_loss_matched = 0.0
        correct_mismatch = 0
        total_mismatch = 0
        correct_matched = 0
        total_matched = 0
        model.eval()
        with torch.no_grad():
            for _, x_forward, x_backward, labels in tqdm(valid_loader_mismatch):
                outputs = model(x_forward.to(device), x_backward.to(device))
                outputs = outputs.to(device)
                outputs = outputs.view(-1, len(vocab))
                target = x_forward.view(-1).to(device)
                loss = criterion(outputs, target)
                valid_loss_mismatch += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total_mismatch += target.size(0)
                correct_mismatch += (predicted == target).sum().item()

            valid_acc_mismatch_epoch = correct_mismatch / total_mismatch
            valid_acc_mismatch.append(valid_acc_mismatch_epoch)
            valid_loss_mismatch /= len(valid_loader_mismatch.dataset)
            valid_losses_mismatch.append(valid_loss_mismatch)

            for _, x_forward, x_backward, labels in tqdm(valid_loader_matched):
                outputs = model(x_forward.to(device), x_backward.to(device))
                outputs = outputs.to(device)
                outputs = outputs.view(-1, len(vocab))
                target = x_forward.view(-1).to(device)
                loss = criterion(outputs, target)
                valid_loss_matched += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                total_matched += target.size(0)
                correct_matched += (predicted == target).sum().item()

            valid_acc_matched_epoch = correct_matched / total_matched
            valid_acc_matched.append(valid_acc_matched_epoch)
            valid_loss_matched /= len(valid_loader_matched.dataset)
            valid_losses_matched.append(valid_loss_matched)

        print(
            f'Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss: .4f}, Valid Loss Mismatch: {valid_loss_mismatch: .4f}, Valid Loss Matched: {valid_loss_matched: .4f}, Train Acc: {train_acc_epoch: .4f}, Valid Acc Mismatch: {valid_acc_mismatch_epoch: .4f}, Valid Acc Matched: { valid_acc_matched_epoch: .4f}')
        
    return train_losses, valid_losses_mismatch, valid_losses_matched, train_acc, valid_acc_mismatch, valid_acc_matched



# Train the ELMo model
# num_epochs=10
# train_losses, valid_losses_mismatch, valid_losses_matched, train_acc, valid_acc_mismatch, valid_acc_matched = train_loop(
#     elmo_model, criterion, optimizer, train_loader, valid_loader_mismatch, valid_loader_matched, num_epochs)

# import matplotlib.pyplot as plt
# # Plot the training and validation losses
# plt.plot(train_losses, label='Training loss')
# plt.plot(valid_losses_mismatch, label='Validation loss mismatch')
# plt.plot(valid_losses_matched, label='Validation loss matched')
# plt.legend(frameon=False)
# plt.show()

# plt.plot(train_acc, label='Training Accuracy')
# plt.plot(valid_acc_mismatch, label='Validation Accuracy Mismatched')
# plt.plot(valid_acc_matched, label='Validation Accuracy Matched')
# plt.legend(frameon=False)
# plt.show()

# torch.save(elmo_model.state_dict(), '/content/drive/MyDrive/NLPA4/elmo_model_nli.pt')
elmo_model.load_state_dict(torch.load(
    './models/elmo_model2.pt', map_location=torch.device('cpu')))
elmo_embeddings = list(elmo_model.parameters())[0].cpu().detach().numpy()
# print(elmo_embeddings.shape)
# word = "<UNK>"
# word_index = vocab[word]
# print(elmo_embeddings[word_index])
# torch.save(elmo_embeddings, '/content/drive/MyDrive/NLPA4/elmo_embeddings_nli.pt')

elmo_lstmf = elmo_model.lstm_forward
elmo_lstmb = elmo_model.lstm_backward

class NLI(nn.Module):
    def __init__(self, vocab_size, elmo_embeddings, embedding_dim=100, hidden_size=100, num_layers=2, dropout=0.2, num_classes=3):
        super(NLI, self).__init__()
        # Embedding layer
        self.embedding = nn.Embedding.from_pretrained(
            torch.tensor(elmo_embeddings))
        self.embedding.weight.requires_grad = True
        self.weightage = nn.Parameter(torch.FloatTensor([0.33, 0.33, 0.33]))

        # LSTM layers
        self.lstm1 = elmo_lstmf
        self.lstm2 = elmo_lstmb

        # Dropout layer
        self.dropout = nn.Dropout(p=dropout)

        # Fully connected layer
        self.fc1 = nn.Linear(embedding_dim, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 3)

    def forward(self, x):
        # Embedding layer
        x = self.embedding(x)
        embed1 = self.fc1(x)
        embed2, _ = self.lstm1(x)
        embed3, _ = self.lstm2(embed2)
        embed = torch.stack([embed1, embed2, embed3], dim=1)
        # perform weighted sum along dim=1
        embed = torch.sum(embed * self.weightage.view(1, -1, 1, 1), dim=1)

        # take max and dropout
        out = torch.max(embed, dim=1)[0]
        out = self.dropout(out)

        # Fully connected layer
        out = self.fc2(out)

        return out


# Instantiate the Sentimental Analysis model
nli_model = NLI(len(vocab), elmo_embeddings).to(device)
print(nli_model)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(nli_model.parameters(), lr=0.001)

import time
from tqdm import tqdm
start_time = time.time()


def train_loop(nli_model, criterion, optimizer, train_loader, valid_loader_mismatch, valid_loader_matched, num_epochs):
    train_losses = []
    valid_losses_mismatch = []
    valid_losses_matched = []

    train_acc = []
    valid_acc_mismatch = []
    valid_acc_matched = []

    for epoch in tqdm(range(num_epochs)):
        # Training
        train_loss = 0.0
        train_acc_ = 0.0
        nli_model.train()
        for i, (x, x_forward, x_backward, labels) in (enumerate(train_loader)):
            optimizer.zero_grad()
            outputs = nli_model(x.to(device))
            outputs = outputs.to(device)
            _, preds = torch.max(outputs, 1)
            labels = labels.to(device)
            # print(labels)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            train_acc_ += torch.sum(preds == labels.data)

        train_loss /= len(train_loader.dataset)
        train_losses.append(train_loss)
        train_acc.append(train_acc_ / len(train_loader.dataset))

        # print losses, acc at every 1000th iteration
        if (i+1) % 1000 == 0:
            print(
                f'Iteration {i+1}/{len(train_loader)}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc_ / len(train_loader.dataset):.4f}')
            
        # Validation
        with torch.no_grad():
            valid_loss_mismatch = 0.0
            valid_acc_mismatch_ = 0.0
            nli_model.eval()
            for x, x_forward, x_backward, labels in (valid_loader_mismatch):
                outputs = nli_model(x.to(device))
                outputs = outputs.to(device)
                _, preds = torch.max(outputs, 1)
                labels = labels.to(device)
                loss = criterion(outputs, labels)
                valid_loss_mismatch += loss.item()
                valid_acc_mismatch_ += torch.sum(preds == labels.data)
            valid_loss_mismatch /= len(valid_loader_mismatch.dataset)
            valid_losses_mismatch.append(valid_loss_mismatch)
            valid_acc_mismatch.append(valid_acc_mismatch_ /
                                      len(valid_loader_mismatch.dataset))

            valid_loss_matched = 0.0
            valid_acc_matched_ = 0.0
            nli_model.eval()
            for x, x_forward, x_backward, labels in tqdm(valid_loader_matched):
                outputs = nli_model(x.to(device))
                outputs = outputs.to(device)
                _, preds = torch.max(outputs, 1)
                labels = labels.to(device)
                loss = criterion(outputs, labels)
                valid_loss_matched += loss.item()
                valid_acc_matched_ += torch.sum(preds == labels.data)
            valid_loss_matched /= len(valid_loader_matched.dataset)
            valid_losses_matched.append(valid_loss_matched)
            valid_acc_matched.append(valid_acc_matched_ /
                                      len(valid_loader_matched.dataset))
            
            print(
                f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc_ / len(train_loader.dataset):.4f}, Valid Loss Mismatch: {valid_loss_mismatch:.4f}, Valid Acc Mismatch: {valid_acc_mismatch_ / len(valid_loader_mismatch.dataset):.4f}, Valid Loss Matched: {valid_loss_matched:.4f}, Valid Acc Matched: {valid_acc_matched_ / len(valid_loader_matched.dataset):.4f}')
            
    return train_losses, valid_losses_mismatch, valid_losses_matched, train_acc, valid_acc_mismatch, valid_acc_matched

# Train the NLI model
# num_epochs = 10
# train_losses, valid_losses_mismatch, valid_losses_matched, train_acc, valid_mismatch_acc, valid_match_acc = train_loop(
#     nli_model, criterion, optimizer, train_loader, valid_loader_mismatch, valid_loader_matched, num_epochs)

end_time = time.time()
print(f'Time taken to run: {end_time - start_time:.2f} seconds')

# torch.save(nli_model.state_dict(), '/content/drive/MyDrive/NLPA4/nli_model.pt')
nli_model.load_state_dict(torch.load(
    './models/nli_model.pt', map_location=torch.device('cpu')))

# # Move tensors to CPU before plotting
# train_acc = [x.cpu().item() for x in train_acc]
# valid_mismatch_acc = [x.cpu().item() for x in valid_mismatch_acc]
# valid_match_acc = [x.cpu().item() for x in valid_match_acc]

# # Plot the training and validation losses
# plt.plot(train_losses, label='Training loss')
# plt.plot(valid_losses_mismatch, label='Validation loss mismatch')
# plt.plot(valid_losses_matched, label='Validation loss matched')
# plt.legend(frameon=False)
# plt.show()

# # Plot the training and validation accuracies
# plt.plot(train_acc, label='Training accuracy')
# plt.plot(valid_mismatch_acc, label='Validation accuracy mismatch')
# plt.plot(valid_match_acc, label='Validation accuracy matched')
# plt.legend(frameon=False)
# plt.show()

def test_loop(nli_model, test_loader):
    test_acc = 0.0
    y_pred = []
    y_true = []
    nli_model.eval()
    with torch.no_grad():
        for x, x_forward, x_backward, labels in tqdm(test_loader):
            outputs = nli_model(x.to(device))
            outputs = outputs.to(device)
            _, preds = torch.max(outputs, 1)
            labels = labels.to(device)
            test_acc += torch.sum(preds == labels.data)
            y_pred.extend(preds.tolist())
            y_true.extend(labels.tolist())
        test_acc /= len(test_loader.dataset)
        print(f'Test Acc: {test_acc:.4f}')
    return y_pred, y_true
y_pred, y_true = test_loop(nli_model, test_loader)