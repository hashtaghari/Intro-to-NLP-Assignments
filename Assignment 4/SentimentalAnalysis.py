# -*- coding: utf-8 -*-
# """1_25pm.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1f7-uAOVQBop3QgejqnxXJJgyO7Y3wtkR
# """

# #mount to google drive
# from google.colab import drive
# drive.mount('/content/drive')

# !pip install datasets

import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from datasets import load_dataset
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torchtext
# Importing Dataset and DataLoader
from torch.utils.data import Dataset, DataLoader
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
dataset = load_dataset("sst")
punctuations = set(string.punctuation)

train = []
for example in dataset["train"]:
    sentence = example["sentence"].lower()
    tokens = word_tokenize(sentence)
    tokens = [(token) for token in tokens if token not in stop_words and token not in punctuations and token.isalnum()]
    train.append({"sentence": tokens, "label": example["label"]})

test = []
for example in dataset["test"]:
    sentence = example["sentence"].lower()
    tokens = word_tokenize(sentence)
    tokens = [(token) for token in tokens if token not in stop_words and token not in punctuations and token.isalnum()]
    test.append({"sentence": tokens, "label": example["label"]})

val = []
for example in dataset["validation"]:
    sentence = example["sentence"].lower()
    tokens = word_tokenize(sentence)
    tokens = [(token) for token in tokens if token not in stop_words and token not in punctuations and token.isalnum()]
    val.append({"sentence": tokens, "label": example["label"]})

def create_vocab(tokenized_dataset, freq_threshold=3):
    # Create a dictionary to hold the word frequency counts
    word_counts = {}

    # Count the frequency of each word in the dataset
    for example in tokenized_dataset:
        for word in example['sentence']:
            if word in word_counts:
                word_counts[word] += 1
            else:
                word_counts[word] = 1

    # Create a vocabulary of words that occur at least freq_threshold times
    vocab = {'<PAD>': 0, '<UNK>': 1}
    index = 2
    for word, count in word_counts.items():
        if count >= freq_threshold:
            vocab[word] = index
            index += 1

    return vocab


def token2index_dataset(tokenized_dataset, vocab):
    # Convert each sentence to a list of indices using the vocabulary
    indexed_dataset = []
    for example in tokenized_dataset:
        indexed_sentence = [vocab.get(word, 1) for word in example['sentence']]
        indexed_dataset.append(
            {'sentence': indexed_sentence, 'label': example['label']})

    return indexed_dataset


# Create the vocabulary
vocab = create_vocab(train)

# Convert the tokenized datasets to indices
train_data = token2index_dataset(train, vocab)
valid_data = token2index_dataset(val, vocab)
test_data = token2index_dataset(test, vocab)

MAX_LENGTH = 100
def pad_sentences(sentences, max_length=100, pad_index=0):
    padded_sentences = []
    for sentence in sentences:
        if len(sentence) >= max_length:
            padded_sentences.append(sentence[:max_length])
        else:
            num_padding = max_length - len(sentence)
            new_sentence = sentence + [pad_index] * num_padding
            padded_sentences.append(new_sentence)
    return padded_sentences

train_data_labels = [example['label'] for example in train_data]
train_data_sentences = [example['sentence'] for example in train_data]
train_data_sentences = pad_sentences(train_data_sentences, MAX_LENGTH)

valid_data_labels = [example['label'] for example in valid_data]
valid_data_sentences = [example['sentence'] for example in valid_data]
valid_data_sentences = pad_sentences(valid_data_sentences, MAX_LENGTH)

test_data_labels = [example['label'] for example in test_data]
test_data_sentences = [example['sentence'] for example in test_data]
test_data_sentences = pad_sentences(test_data_sentences, MAX_LENGTH)

class DatasetConstructor(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = [0 if label < 0.5 else 1 for label in labels]
        self.backward_data = [sentence[:-1] for sentence in self.data]
        self.forward_data = [sentence[1:] for sentence in self.data]

    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, index):
        return torch.tensor(self.data[index]) ,torch.tensor(self.forward_data[index]), torch.tensor(self.backward_data[index]), torch.tensor(self.labels[index])
    

train_dataset = DatasetConstructor(train_data_sentences, train_data_labels)
valid_dataset = DatasetConstructor(valid_data_sentences, valid_data_labels)
test_dataset = DatasetConstructor(test_data_sentences, test_data_labels)

# create data loaders
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

# Load the GloVe embeddings
glove_vectors = torchtext.vocab.GloVe(name='6B', dim=100)

# Create a weight matrix for words in the training dataset
weights_matrix = torch.zeros((len(vocab), 100))
words_found = 0
for i, word in enumerate(vocab.keys()):
    try:
        weights_matrix[i] = glove_vectors[word]
        words_found += 1
    except KeyError:
        weights_matrix[i] = torch.zeros(100)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import torch.nn as nn
import torch.nn.functional as F


class ELMo(nn.Module):
    def __init__(self, vocab_size, weights_matrix, hidden_size=100, num_layers=2, dropout=0.2):
        super(ELMo, self).__init__()

        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, 100)
        self.embedding.weight.data.copy_(torch.FloatTensor(weights_matrix))
        self.embedding.weight.requires_grad = True

        # LSTM layers
        self.lstm_forward = nn.LSTM(
            100, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)
        self.lstm_backward = nn.LSTM(
            100, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)

        # Dropout layer
        self.dropout = nn.Dropout(p=dropout)

        # Fully connected layer
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x_forward, x_backward):
        # Embedding layer
        x_forward = self.embedding(x_forward)
        x_backward = self.embedding(x_backward)

        # print("1 ", x_backward.shape)

        # LSTM layers
        out_forward, _ = self.lstm_forward(x_backward)
        # print("2 ", out_forward.shape)
        out_backward, _ = self.lstm_backward(out_forward)
        # print("3 ", out_backward.shape)

        out = self.fc(out_backward)

        return out



# Instantiate the ELMo model
elmo_model = ELMo(len(vocab), weights_matrix).to(device)
print(elmo_model)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(elmo_model.parameters(), lr=0.001)


## PRETRAINING
from tqdm import tqdm
# Define the training loop
def train_loop(model, criterion, optimizer, train_loader, valid_loader, num_epochs):
    train_losses = []
    valid_losses = []
    for epoch in tqdm(range(num_epochs)):
        # Train the model
        train_loss = 0.0
        model.train()
        for _, x_forward, x_backward, labels in tqdm(train_loader):
            # print(target.shape)
            optimizer.zero_grad()
            outputs = model(x_forward.to(device), x_backward.to(device))
            outputs = outputs.to(device)
            outputs = outputs.view(-1, len(vocab))
            target = x_forward.view(-1).to(device)
            loss = criterion(outputs, target)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader.dataset)
        train_losses.append(train_loss)

        # Evaluate the model
        valid_loss = 0.0
        model.eval()
        with torch.no_grad():
            for _, x_forward, x_backward, labels in valid_loader:
                outputs = model(x_forward.to(device), x_backward.to(device))
                outputs = outputs.to(device)
                target = x_forward.view(-1).to(device)
                outputs = outputs.view(-1, len(vocab))
                loss = criterion(outputs, target)
                valid_loss += loss.item()
            valid_loss /= len(valid_loader.dataset)
            valid_losses.append(valid_loss)

        print(
            f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')

    return train_losses, valid_losses


# Train the ELMo model
num_epochs = 10
# train_losses, valid_losses = train_loop(
#     elmo_model, criterion, optimizer, train_loader, valid_loader, num_epochs)

# import matplotlib.pyplot as plt
# # Plot the training and validation loss
# plt.plot(train_losses, label='Training loss')
# plt.plot(valid_losses, label='Validation loss')
# plt.legend()
# plt.show()

# save the model
# torch.save(elmo_model.state_dict(), '/content/drive/MyDrive/NLPA4/elmo_model1.pt')

# elmo_model.load_state_dict(torch.load('./models/elmo_model1.pt'))
elmo_model.load_state_dict(torch.load(
    './models/elmo_model1.pt', map_location=torch.device('cpu')))


# for name, param in elmo_model.named_parameters():
#     if param.requires_grad:
#         print(name, param.data, param.shape)

elmo_embeddings = list(elmo_model.parameters())[0].cpu().detach().numpy()
# print(elmo_embeddings.shape)
# word = "<UNK>"
# word_index = vocab[word]
# print(elmo_embeddings[word_index])
# torch.save(elmo_embeddings, './models/elmo_embeddings1.pt')

elmo_lstmf = elmo_model.lstm_forward
elmo_lstmb = elmo_model.lstm_backward

# Sentimental Analysis Model
class SentimentalAnalysis(nn.Module):
    def __init__(self, vocab_size, elmo_embeddings, embedding_dim=100, hidden_size=100, num_layers=2, dropout=0.2, num_classes=2):
        super(SentimentalAnalysis, self).__init__()
        # Embedding layer
        self.embedding = nn.Embedding.from_pretrained(
            torch.tensor(elmo_embeddings))
        self.embedding.weight.requires_grad = True
        self.weightage = nn.Parameter(torch.FloatTensor([0.33, 0.33, 0.33]))

        # LSTM layers
        self.lstm1 = elmo_lstmf
        self.lstm2 = elmo_lstmb

        # Dropout layer
        self.dropout = nn.Dropout(p=dropout)

        # Fully connected layer
        self.fc1 = nn.Linear(embedding_dim, hidden_size)
        self.fc2 = nn.Linear(hidden_size, 2)

    def forward(self, x):
        # Embedding layer
        x = self.embedding(x)
        embed1 = self.fc1(x)
        embed2, _ = self.lstm1(x)
        embed3, _ = self.lstm2(embed2)
        embed = torch.stack([embed1, embed2, embed3], dim=1)
        embed = torch.sum(embed * self.weightage.view(1, -1, 1, 1), dim=1)  # perform weighted sum along dim=1

        # take max and dropout
        out = torch.max(embed, dim=1)[0]
        out = self.dropout(out)

        # Fully connected layer
        out = self.fc2(out)

        return out



# Instantiate the Sentimental Analysis model
sentimental_model = SentimentalAnalysis(len(vocab), elmo_embeddings).to(device)
print(sentimental_model)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(sentimental_model.parameters(), lr=0.001)


## TRAIN LOOP
# Define the training loop
def train_loop(model, criterion, optimizer, train_loader, valid_loader, num_epochs):
    train_losses = []
    valid_losses = []
    train_acc = []
    valid_acc = []
    for epoch in tqdm(range(num_epochs)):
        # Train the model
        train_loss = 0.0
        train_correct = 0
        model.train()
        for x, x_forward, x_backward, labels in tqdm(train_loader):
            # print(target.shape)
            optimizer.zero_grad()
            outputs = model(x.to(device))
            outputs = outputs.to(device)
            labels = labels.to(device)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            train_correct += (torch.argmax(outputs, 1) == labels).sum().item()
        train_loss /= len(train_loader.dataset)
        train_losses.append(train_loss)
        train_acc.append(train_correct/len(train_loader.dataset))

        # Evaluate the model
        valid_loss = 0.0
        valid_correct = 0
        model.eval()
        with torch.no_grad():
            for x, x_forward, x_backward, labels in valid_loader:
                outputs = model(x.to(device))
                outputs = outputs.to(device)
                labels = labels.to(device)
                loss = criterion(outputs, labels)
                valid_loss += loss.item()
                valid_correct += (torch.argmax(outputs, 1) == labels).sum().item()
            valid_loss /= len(valid_loader.dataset)
            valid_losses.append(valid_loss)
            valid_acc.append(valid_correct/len(valid_loader.dataset))

        print(
            f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}, Training Accuracy: {train_correct/len(train_loader.dataset):.4f}, Validation Accuracy: {valid_correct/len(valid_loader.dataset):.4f}')
    
    return train_losses, valid_losses, train_acc, valid_acc


# # Train the Sentimental Analysis model
# num_epochs = 10
# train_losses, valid_losses, train_acc, valid_acc = train_loop(
#     sentimental_model, criterion, optimizer, train_loader, valid_loader, num_epochs)

# # Plot the training and validation loss
# plt.plot(train_losses, label='Training loss')
# plt.plot(valid_losses, label='Validation loss')
# plt.legend()
# plt.show()

# # Plot the training and validation accuracy
# plt.plot(train_acc, label='Training accuracy')
# plt.plot(valid_acc, label='Validation accuracy')
# plt.legend()
# plt.show()

# save the model
# torch.save(sentimental_model.state_dict(),
#            './models/sentimental_model.pt')

sentimental_model.load_state_dict(torch.load('./models/sentimental_model.pt', map_location=torch.device('cpu')))

# plot accuracy
def accuracy(model, data_loader):
    correct_pred, num_examples = 0, 0
    model.eval()
    for x, _, _, labels in tqdm(data_loader):
        outputs = model(x.to(device))
        outputs = outputs.to(device)
        predicted_labels = torch.argmax(outputs, 1)
        num_examples += labels.size(0)
        correct_pred += (predicted_labels == labels.to(device)).sum()
    return correct_pred.float()/num_examples * 100


print(f'Test accuracy: {accuracy(sentimental_model, test_loader):.2f}%')