# -*- coding: utf-8 -*-
"""CBOW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1htaJTbjAOo-sUgMJ8C-rERP5MfC9aHga
"""

# -*- coding: utf-8 -*-
"""CBOW.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cSI7aO1WqIjtMXjnFclEdPVR0R_tSggb
"""

from google.colab import drive
drive.mount('/content/drive')

import nltk
import numpy as np
from nltk.corpus import gutenberg
from string import punctuation
import re
from keras.preprocessing import text
import pandas as pd
from keras.utils import pad_sequences
from keras.utils import np_utils
import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import nltk
nltk.download('stopwords')
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

wpt = nltk.WordPunctTokenizer()
stop_words = nltk.corpus.stopwords.words('english')

def clean_text(doc):
    doc = re.sub(r'[^a-zA-Z\s]', '', doc, re.I|re.A)
    doc = doc.lower().strip()
    tokens = wpt.tokenize(doc)
    filtered_tokens = [token for token in tokens if token not in stop_words]
    doc = ' '.join(filtered_tokens)
    return doc

normalize_corpus = np.vectorize(clean_text)

data_path = "/content/drive/MyDrive/NLP A3/reviews_Movies_and_TV.json"

# load only 100000 entries
df = pd.read_json(data_path, lines=True, nrows=100000)
# only keep the reviewText
df = df['reviewText']
# tokenize
df = df.apply(clean_text)
# convert to list
df = df.tolist()
review_data = df

# remove words that occur less than 4 times
from collections import Counter
counts = Counter()
for review in review_data:
    for word in review.split():
        counts[word] += 1

# remove words that occur less than 4 times
review_data = [' '.join([word for word in review.split() if counts[word] >= 4]) for review in review_data]

# save the data in a file
with open('./reviews.txt', 'w') as f:
    for review in review_data:
        f.write(review + '\n')

tokenizer = text.Tokenizer()
tokenizer.fit_on_texts(review_data)
word2id = tokenizer.word_index
word2id['PAD'] = 0
id2word = {idx: word for word, idx in word2id.items()}
wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in review_data]
vocab_size = len(word2id)

# save the tokenized data
with open('./reviews_tokenized.txt', 'w') as f:
    for review in tokenizer.texts_to_sequences(review_data):
        f.write(str(review) + '\n')


embed_size = 100
window_size = 2

X = []
Y = []
context_length = window_size*2
for words in wids:
    sentence_length = len(words)
    for index, word in enumerate(words):           
        start = index - window_size
        end = index + window_size + 1
        context = [words[i] for i in range(start, end)if 0 <= i < sentence_length and i != index]
        x = pad_sequences([context], maxlen=context_length)
        X.append(x)
        Y.append(word)

# reduce size
X = X[:10000]
Y = Y[:10000]
# save the data
with open('./X.txt', 'w') as f:
    for review in X:
        f.write(str(review) + '\n')

with open('./Y.txt', 'w') as f:
    for review in Y:
        f.write(str(review) + '\n')

class CBOW(torch.nn.Module):

    def __init__(self):
        super(CBOW, self).__init__()
        self.embeddings = nn.Embedding(vocab_size, embed_size).to(device)
        self.linear1 = nn.Linear(embed_size, 100).to(device)
        self.activation_function1 = nn.ReLU().to(device)        
        self.linear2 = nn.Linear(100, vocab_size).to(device)
        self.activation_function2 = nn.LogSoftmax(dim = -1).to(device)
        

    def forward(self, inputs):
        embeds = sum(self.embeddings(torch.from_numpy(inputs).long().to(device))).view(1,-1).to(device)
        out = self.linear1(embeds).to(device)
        out = self.activation_function1(out).to(device)
        out = self.linear2(out).to(device)
        out = self.activation_function2(out).to(device)
        return out.to(device)
    
model = CBOW()
model.to(device)

loss_function = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.SGD(model.parameters(), lr=0.05)

# train the model
# show progress bar

from tqdm import tqdm

for epoch in range(1, 5):
    total_loss = 0.
    num_examples = 0
    with tqdm(total=len(X)) as progress_bar:
        for x, y in zip(X, Y):
            optimizer.zero_grad()
            log_probs = model(x[0])
            loss = loss_function(log_probs, torch.Tensor([y]).long().to(device)).to(device)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            num_examples += 1
            progress_bar.update(1)
            if(num_examples%1000==0):
              avg_loss = total_loss / num_examples
              print(f'Epoch: {epoch}, iterations: {num_examples}, \tLoss: {avg_loss:.4f}')
        avg_loss = total_loss / num_examples
        print(f'Epoch: {epoch}\tLoss: {avg_loss:.4f}')

# save the model
torch.save(model.state_dict(), '/content/drive/MyDrive/NLP A3/CBOW/cbow_model.pt')

# load the model
model.load_state_dict(torch.load('/content/drive/MyDrive/NLP A3/CBOW/cbow_model.pt'))
weights = model.embeddings(torch.Tensor([list(range(0,vocab_size))]).long().to(device))
pd.DataFrame(weights.view(-1,100).tolist(), index=list(id2word.values())[0:]).head()

import numpy as np
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# get top 10 closest words to:
words = ["sometimes", "know", "media", "china", "pleased"]

# use cosine similarity to find top 10 closest words for each word in words
top_words = {}
for word in words:
    # get cosine similarity between word vector and all other word vectors
    sim = cosine_similarity(weights[0][word2id[word]].view(1,-1).detach().cpu(), weights[0].detach().cpu()).flatten()
    # find the indices of the top 10 highest similarity scores (excluding the word itself)
    top_word_indices = sim.argsort()[-11:-1][::-1]
    # get the corresponding words
    top_words[word] = [id2word[i] for i in top_word_indices]

# concatenate all the top words together and get their vectors
top_word_vectors = []
for word in words:
    top_word_vectors.extend([weights[0][word2id[w]] for w in top_words[word]])
top_word_vectors = torch.stack(top_word_vectors).detach().cpu()

# reduce dimensionality of word vectors using TSNE
tsne = TSNE(n_components=2, random_state=0)
word_vectors_tsne = tsne.fit_transform(top_word_vectors)

# plot the results
fig, ax = plt.subplots(figsize=(10,10))
for i, word in enumerate(words):
    # get the indices of the words for this group
    start = i * 10
    end = (i + 1) * 10
    # plot the words
    ax.scatter(word_vectors_tsne[start:end,0], word_vectors_tsne[start:end,1], label=word)
    for j, w in enumerate(top_words[word]):
        ax.annotate(w, (word_vectors_tsne[start+j,0], word_vectors_tsne[start+j,1]))
ax.legend()
plt.show()

# get top 10 closest words to "titanic"
word = "titanic"
# get cosine similarity between word vector and all other word vectors
sim = cosine_similarity(weights[0][word2id[word]].view(
    1, -1).detach().cpu(), weights[0].detach().cpu()).flatten()
# find the indices of the top 10 highest similarity scores (excluding the word itself)
top_word_indices = sim.argsort()[-11:-1][::-1]
# get the corresponding words
top_words = [id2word[i] for i in top_word_indices]

# concatenate all the top words together and get their vectors
top_word_vectors = [weights[0][word2id[w]] for w in top_words]
top_word_vectors = torch.stack(top_word_vectors).detach().cpu()

# reduce dimensionality of word vectors using TSNE
tsne = TSNE(n_components=2, random_state=0, perplexity=5)
word_vectors_tsne = tsne.fit_transform(top_word_vectors)

# plot the results
fig, ax = plt.subplots(figsize=(10, 10))
ax.scatter(word_vectors_tsne[:, 0], word_vectors_tsne[:, 1])
for i, w in enumerate(top_words):
    ax.annotate(w, (word_vectors_tsne[i, 0], word_vectors_tsne[i, 1]))
plt.show()